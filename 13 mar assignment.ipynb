{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7071b6-a4dc-40b8-9ca6-33e16b28b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ecd4a-0bc4-4678-8ffd-5c4f95d92038",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine if there are any significant differences among them. However, ANOVA comes with certain assumptions that need to be met for its results to be valid. Violations of these assumptions can lead to inaccurate or unreliable results. The main assumptions of ANOVA are:\n",
    "\n",
    "Independence: The observations within each group are assumed to be independent of each other. This means that the values in one group should not be related or influenced by the values in another group.\n",
    "\n",
    "Normality: The distribution of the residuals (the differences between observed values and predicted values) within each group should be approximately normal. This assumption is particularly important when sample sizes are small.\n",
    "\n",
    "Homogeneity of Variances (Homoscedasticity): The variability of the dependent variable should be roughly the same across all groups. In other words, the variance of the residuals should be constant across groups.\n",
    "\n",
    "Now, let's discuss examples of violations for each assumption:\n",
    "\n",
    "Independence:\n",
    "\n",
    "Violation Example: In a study comparing the performance of students from different schools, if students from the same school are more similar to each other than to students from other schools, independence is violated.\n",
    "Impact: Violation of independence can lead to pseudoreplication, where the same underlying factors contribute to both groups, making it difficult to determine if observed differences are due to the factor of interest.\n",
    "Normality:\n",
    "\n",
    "Violation Example: In a study comparing the reaction times of different age groups, if the reaction times within each group are not normally distributed, normality assumption is violated.\n",
    "Impact: Non-normality can distort the p-values and confidence intervals, potentially leading to incorrect conclusions about the significance of group differences.\n",
    "Homogeneity of Variances:\n",
    "\n",
    "Violation Example: In an experiment comparing the yield of crops grown using different fertilizers, if the variability of yields is much larger in one group compared to others, homogeneity of variances is violated.\n",
    "Impact: Violation of homoscedasticity can lead to unequal influence of groups on the ANOVA results. It may also affect the validity of the F-test used in ANOVA.\n",
    "It's important to note that ANOVA is relatively robust to violations of assumptions, especially when sample sizes are large. In some cases, transformations of the data or using non-parametric alternatives might help mitigate violations. However, if assumptions are severely violated, alternative analysis methods may be more appropriate, such as Welch's ANOVA or non-parametric tests like the Kruskal-Wallis test.\n",
    "\n",
    "Researchers should always be cautious when interpreting ANOVA results, especially if assumptions are violated, and consider conducting further analyses or using alternative methods to ensure the validity of their conclusions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a16e38d9-93e5-48b5-a337-33856281ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce065488-4bcb-4375-b772-d5d447cadd04",
   "metadata": {},
   "source": [
    "There are three main types of Analysis of Variance (ANOVA), each designed for specific situations and research designs:\n",
    "\n",
    "One-Way ANOVA:\n",
    "\n",
    "Situation: One-Way ANOVA is used when you have one independent variable (factor) with three or more levels (groups) and you want to compare the means of these groups to determine if there are any significant differences.\n",
    "Example: Suppose you are studying the effect of different teaching methods (A, B, and C) on student test scores. You have three groups of students, each taught using a different method, and you want to know if there is a significant difference in mean test scores among the groups.\n",
    "Two-Way ANOVA:\n",
    "\n",
    "Situation: Two-Way ANOVA is used when you have two independent variables (factors) and you want to examine how their interaction affects the dependent variable. This type of ANOVA helps you understand if there are main effects of each factor and if there is an interaction effect between them.\n",
    "Example: Imagine you are conducting an experiment to investigate the effects of both diet type (low-fat, high-fat) and exercise level (sedentary, active) on weight loss. Two-Way ANOVA would allow you to analyze how these two factors independently and together influence weight loss.\n",
    "Repeated Measures ANOVA:\n",
    "\n",
    "Situation: Repeated Measures ANOVA is used when you have a single group of subjects and you measure them repeatedly under different conditions. This type of ANOVA is suitable for within-subject designs where each subject is measured multiple times under different treatment conditions.\n",
    "Example: You are studying the effect of a new drug on blood pressure and measure the blood pressure of the same group of individuals before and after taking the drug. Repeated Measures ANOVA would help you analyze whether there are significant differences in blood pressure across the different time points.\n",
    "It's important to choose the appropriate type of ANOVA based on your research design and the nature of your data. Selecting the wrong type of ANOVA can lead to incorrect conclusions or inefficient use of statistical methods. Additionally, when performing any type of ANOVA, it's crucial to ensure that the assumptions of ANOVA are met or appropriately addressed to ensure the validity of the results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69981a4e-6692-4565-9dab-400a715bd8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdd9205-0649-431a-a12a-5406bc6646f4",
   "metadata": {},
   "source": [
    "Partitioning of variance in ANOVA refers to the process of decomposing the total variability in a dataset into different sources of variation. This breakdown allows us to understand how much of the overall variability is attributed to different factors or sources, such as treatment effects, experimental error, or interactions. The concept of partitioning of variance is fundamental to ANOVA because it provides valuable insights into the contributions of various factors to the observed differences in the data.\n",
    "\n",
    "In ANOVA, the total variance is split into two main components:\n",
    "\n",
    "Between-Groups Variance (Treatment Variance): This represents the variability in the data that can be attributed to the differences between the various groups or treatments being compared. It reflects the effect of the factor of interest (e.g., different teaching methods, different drug doses) on the dependent variable.\n",
    "\n",
    "Within-Groups Variance (Error Variance): This represents the variability in the data that is not accounted for by the factor of interest. It includes random variability and experimental error within each group.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "Identifying Sources of Variation: By partitioning the variance, ANOVA helps researchers identify and quantify the contributions of different factors to the variability in the data. This information is crucial for determining whether the observed differences are statistically significant and for understanding the relative importance of various factors.\n",
    "\n",
    "Hypothesis Testing: ANOVA uses the ratio of between-groups variance to within-groups variance (F-ratio) to perform hypothesis testing. A large F-ratio suggests that the treatment effects are significant, while a small F-ratio suggests that the differences between groups are not statistically significant.\n",
    "\n",
    "Interpreting Results: Partitioning of variance provides insights into the magnitude and direction of effects. Researchers can infer whether the factor being studied has a substantial impact on the dependent variable based on the proportion of variance it explains.\n",
    "\n",
    "Experimental Design: Understanding the contributions of different sources of variation can help researchers refine their experimental designs, control for confounding variables, and improve the overall quality of their studies.\n",
    "\n",
    "Generalizability: When planning future studies or making decisions based on the results, understanding the partitioning of variance can help in predicting how well the observed effects will generalize to broader populations or contexts.\n",
    "\n",
    "Overall, partitioning of variance in ANOVA helps researchers make informed decisions about the significance of their findings, aids in experimental design, and enhances the understanding of the relationships between different factors and the dependent variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f00fec3-3866-4cbd-986e-584f64936177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "657949bb-f181-4fc2-bbf4-02b5e8ec5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (0.13.5)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (1.23.5)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (22.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->statsmodels) (2022.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79408f0-5ca8-484a-949b-1429ebd6a665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sum of Squares (SST): 3264.933333333333\n",
      "Explained Sum of Squares (SSE): 2980.9333333333343\n",
      "Residual Sum of Squares (SSR): 284.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for each group\n",
    "group1 = np.array([45, 55, 60, 52, 58])\n",
    "group2 = np.array([70, 75, 82, 68, 74])\n",
    "group3 = np.array([90, 88, 85, 92, 87])\n",
    "\n",
    "# Combine all data into a single array\n",
    "all_data = np.concatenate((group1, group2, group3))\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(all_data)\n",
    "\n",
    "# Calculate the Total Sum of Squares (SST)\n",
    "sst = np.sum((all_data - overall_mean)**2)\n",
    "\n",
    "# Calculate the group means\n",
    "group1_mean = np.mean(group1)\n",
    "group2_mean = np.mean(group2)\n",
    "group3_mean = np.mean(group3)\n",
    "\n",
    "# Calculate the Explained Sum of Squares (SSE)\n",
    "sse = len(group1) * (group1_mean - overall_mean)**2 + \\\n",
    "      len(group2) * (group2_mean - overall_mean)**2 + \\\n",
    "      len(group3) * (group3_mean - overall_mean)**2\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "ssr = np.sum((group1 - group1_mean)**2) + \\\n",
    "      np.sum((group2 - group2_mean)**2) + \\\n",
    "      np.sum((group3 - group3_mean)**2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Total Sum of Squares (SST):\", sst)\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10bf36a-fc43-43e0-a214-378b3d777ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b079ac8a-8a52-4499-a81e-896958068f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     df      sum_sq   mean_sq         F    PR(>F)\n",
      "factor_A            1.0    3.217238  3.217238  0.543980  0.464534\n",
      "factor_B            1.0    0.059160  0.059160  0.010003  0.920767\n",
      "factor_A:factor_B   1.0    7.707991  7.707991  1.303288  0.259522\n",
      "Residual           46.0  272.056099  5.914263       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data for a two-way ANOVA\n",
    "np.random.seed(123)\n",
    "n = 50\n",
    "factor_A = np.repeat(['A1', 'A2'], n//2)\n",
    "factor_B = np.tile(['B1', 'B2'], n//2)\n",
    "response = np.random.normal(loc=10, scale=2, size=n)\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "data = pd.DataFrame({'response': response,\n",
    "                     'factor_A': factor_A,\n",
    "                     'factor_B': factor_B})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('response ~ factor_A + factor_B + factor_A:factor_B', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "482c935e-c066-4657-9301-765d2838d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33555c79-a196-457d-8b4c-2623cc63fff3",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic is used to test whether there are significant differences in means among the groups. The p-value associated with the F-statistic indicates the probability of observing such differences by random chance alone. Here's how you can interpret the given results:\n",
    "\n",
    "F-Statistic: The F-statistic is 5.23. This value represents the ratio of the variance between groups to the variance within groups. In other words, it quantifies the extent to which the means of the groups differ relative to the variability within each group. A larger F-statistic suggests that the differences between the group means are relatively larger compared to the variability within each group.\n",
    "\n",
    "P-Value: The p-value associated with the F-statistic is 0.02. This is the probability of obtaining an F-statistic as extreme as the one calculated if there were no true differences between the group means (i.e., if the null hypothesis were true). A p-value of 0.02 indicates that there is a 2% chance of observing these differences in group means just by random chance, assuming that the null hypothesis is true.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "Given the results:\n",
    "\n",
    "The p-value (0.02) is less than the commonly chosen significance level of 0.05.\n",
    "This suggests that the observed differences between the group means are statistically significant at the 0.05 level.\n",
    "Therefore, you would conclude:\n",
    "\n",
    "There is evidence to reject the null hypothesis.\n",
    "The data provides sufficient evidence to suggest that there are significant differences in means among the groups.\n",
    "In simpler terms:\n",
    "\n",
    "You have found that the groups are not all the same; at least one group differs from the others.\n",
    "The differences you observed are unlikely to have occurred just by random chance.\n",
    "It's important to note that while the p-value indicates statistical significance, it does not provide information about the practical or substantive significance of the differences. Additionally, the interpretation of p-values should always be considered in the context of the specific research question and the assumptions underlying the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53d9a101-b173-497d-b9ab-0e03a7c3a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e5698-63b5-4dac-bbc2-36387d59dbf6",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is important to ensure the validity and reliability of your results. Missing data can arise due to various reasons such as participant dropout, measurement errors, or other factors. There are several methods to handle missing data in a repeated measures ANOVA, each with its own implications. Here are some common methods and their potential consequences:\n",
    "\n",
    "Complete Case Analysis (Listwise Deletion):\n",
    "\n",
    "Method: Exclude any participants with missing data in any of the repeated measures.\n",
    "Consequences: This method can lead to a reduction in sample size and potentially bias the results if the missing data are not completely random. It can also affect the representativeness of the sample.\n",
    "Mean Imputation:\n",
    "\n",
    "Method: Replace missing values with the mean of the observed values for that variable.\n",
    "Consequences: Mean imputation can distort the distribution and relationships among variables. It can also lead to underestimation of standard errors and inflated Type I error rates, potentially resulting in unreliable statistical significance.\n",
    "Last Observation Carried Forward (LOCF):\n",
    "\n",
    "Method: Replace missing values with the last observed value from the same participant.\n",
    "Consequences: LOCF may not accurately reflect the true trajectory of the data, especially if the missingness is non-random. It can lead to biased estimates, particularly if the missingness is related to treatment effects.\n",
    "Linear Interpolation:\n",
    "\n",
    "Method: Use linear interpolation to estimate missing values based on adjacent observed values.\n",
    "Consequences: Linear interpolation assumes a linear relationship between adjacent points, which might not hold true in all cases. It can introduce artificial patterns and inaccuracies in the data.\n",
    "Multiple Imputation:\n",
    "\n",
    "Method: Generate multiple plausible values for missing data and create multiple datasets. Perform analysis on each dataset and combine results.\n",
    "Consequences: Multiple imputation accounts for the uncertainty due to missing data and provides more accurate estimates. However, it can be computationally intensive and requires assumptions about the missing data mechanism.\n",
    "Mixed-Effects Models:\n",
    "\n",
    "Method: Use mixed-effects models (e.g., linear mixed-effects models) that incorporate all available data, including participants with missing data, while accounting for within-subject correlations.\n",
    "Consequences: Mixed-effects models can provide valid estimates even with missing data. However, assumptions about the missing data mechanism and model specifications must be carefully considered.\n",
    "It's important to choose a method that is appropriate for your data and research question, while also considering the assumptions of each method. Additionally, sensitivity analyses or comparing results obtained from different methods can provide insights into the potential impact of missing data handling on your conclusions. Transparent reporting of the missing data handling methods and their potential impact is essential for the interpretation of your findings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67ccfc9b-8e89-42e2-8454-9ac3417c5225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafcede6-53c1-43d8-93fd-23bd83ae7e3b",
   "metadata": {},
   "source": [
    "Post-hoc tests are used after performing an Analysis of Variance (ANOVA) to determine which specific groups differ significantly from each other when a significant overall effect is detected. Since ANOVA itself only tells you that there are differences among groups, post-hoc tests help identify which group(s) are responsible for those differences. There are several post-hoc tests available, and the choice depends on the design of your study and the assumptions you're willing to make. Here are some common post-hoc tests and situations where they might be used:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD):\n",
    "\n",
    "Use: Tukey's HSD is conservative and appropriate when you have a balanced design (equal group sizes) and you want to control the familywise error rate. It compares all possible pairs of means and provides a simultaneous confidence interval for each comparison.\n",
    "Example: In a study comparing the effects of three different diets on weight loss, you find a significant difference in mean weight loss among the diets using ANOVA. To determine which specific diets are significantly different from each other, you would use Tukey's HSD.\n",
    "Bonferroni Correction:\n",
    "\n",
    "Use: The Bonferroni correction is a conservative method that controls the familywise error rate by adjusting the significance level for each individual comparison. It's suitable when you have a large number of pairwise comparisons and want to be cautious about making Type I errors.\n",
    "Example: In a clinical trial with multiple treatment groups, you want to compare each treatment to the control group. Since you have a large number of comparisons, you use Bonferroni correction to control for the increased likelihood of false positives.\n",
    "Dunn's Test:\n",
    "\n",
    "Use: Dunn's test is a non-parametric post-hoc test suitable for situations where the assumptions of ANOVA (normality and homogeneity of variances) are violated. It doesn't assume equal group sizes and is less sensitive to outliers.\n",
    "Example: You conduct an experiment comparing response times of participants under different lighting conditions, and the data is not normally distributed. You use a Kruskal-Wallis ANOVA and perform Dunn's test to compare pairs of lighting conditions.\n",
    "Scheffe's Test:\n",
    "\n",
    "Use: Scheffe's test is less conservative than Tukey's HSD and can be used when you have unequal sample sizes or want to control the familywise error rate over all possible comparisons.\n",
    "Example: In a study with several treatment groups and a control group, you want to make multiple pairwise comparisons to understand which treatments are significantly different from the control group. Scheffe's test can provide broader control over comparisons.\n",
    "Fisher's Least Significant Difference (LSD):\n",
    "\n",
    "Use: Fisher's LSD is more powerful but less conservative than some other methods. It can be used when you have a balanced design and want to perform pairwise comparisons.\n",
    "Example: In a study comparing the effects of different exercise regimens on endurance, you find a significant difference using ANOVA. You want to determine which specific pairs of regimens lead to significant differences in endurance.\n",
    "Remember, the choice of post-hoc test should take into account the assumptions of the test, the design of your study, and the goals of your analysis. Always report which post-hoc test you used, along with the rationale for your choice.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9cdd4a0-432a-40ba-9094-db9dd255b879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "447ae654-eb2f-4142-a4b1-7d9ae7dc8649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 868.6484697041565\n",
      "p-value: 3.7527308231255954e-82\n",
      "There are significant differences in mean weight loss among the diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for weight loss (in pounds) for each diet\n",
    "diet_A = np.array([2.5, 3.0, 1.8, 2.7, 2.3, 2.6, 3.2, 2.9, 2.1, 3.1,\n",
    "                   2.7, 2.8, 3.4, 2.2, 2.5, 2.9, 2.6, 2.7, 2.4, 2.8,\n",
    "                   2.5, 2.0, 2.3, 2.6, 2.9, 2.4, 2.8, 2.6, 3.0, 2.7,\n",
    "                   3.1, 2.2, 2.5, 2.9, 2.7, 2.6, 2.8, 2.3, 2.1, 3.0,\n",
    "                   2.7, 2.4, 2.8, 3.2, 2.6, 2.9, 2.7, 2.5, 2.4, 2.1])\n",
    "\n",
    "diet_B = np.array([3.8, 3.5, 4.1, 4.0, 3.9, 4.2, 3.7, 3.6, 3.3, 4.1,\n",
    "                   3.9, 4.2, 3.7, 3.5, 4.0, 4.3, 3.8, 3.9, 3.6, 4.1,\n",
    "                   4.0, 3.7, 3.5, 4.2, 4.1, 3.8, 3.9, 3.6, 4.3, 3.7,\n",
    "                   3.5, 4.0, 4.1, 3.8, 3.9, 4.2, 3.7, 3.6, 3.3, 4.1,\n",
    "                   3.9, 4.2, 3.7, 3.5, 4.0, 4.3, 3.8, 3.9, 3.6, 4.1])\n",
    "\n",
    "diet_C = np.array([1.5, 1.8, 1.3, 1.7, 1.4, 1.6, 1.9, 1.7, 1.4, 1.8,\n",
    "                   1.5, 1.6, 1.2, 1.7, 1.9, 1.6, 1.5, 1.4, 1.7, 1.8,\n",
    "                   1.3, 1.5, 1.6, 1.7, 1.9, 1.2, 1.4, 1.7, 1.6, 1.5,\n",
    "                   1.8, 1.9, 1.6, 1.5, 1.3, 1.7, 1.4, 1.6, 1.9, 1.7,\n",
    "                   1.5, 1.2, 1.4, 1.7, 1.6, 1.5, 1.8, 1.9, 1.7, 1.4])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"There are significant differences in mean weight loss among the diets.\")\n",
    "else:\n",
    "    print(\"There are no significant differences in mean weight loss among the diets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fccb2b5-5114-4517-96a3-84d1af79dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49f30d16-893e-45be-8d33-cfa526f8b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      df      sum_sq   mean_sq         F    PR(>F)\n",
      "Program              2.0    9.204202  4.602101  0.872069  0.430918\n",
      "Experience           1.0    0.855073  0.855073  0.162031  0.690856\n",
      "Program:Experience   2.0    8.123378  4.061689  0.769665  0.474265\n",
      "Residual            24.0  126.653259  5.277219       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(123)\n",
    "n = 30\n",
    "programs = np.random.choice(['A', 'B', 'C'], size=n)\n",
    "experience = np.random.choice(['Novice', 'Experienced'], size=n)\n",
    "times = np.random.normal(loc=10, scale=2, size=n)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({'Program': programs, 'Experience': experience, 'Time': times})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ Program * Experience', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b2825-e877-4268-ab3a-58a1914532c5",
   "metadata": {},
   "source": [
    "To interpret the results:\n",
    "\n",
    "Look at the p-values associated with the main effects (Program, Experience) and the interaction effect (Program * Experience).\n",
    "If a p-value is less than your chosen significance level (e.g., 0.05), you have evidence to reject the null hypothesis for that effect.\n",
    "If both main effects and the interaction effect have small p-values, you can conclude that there are significant main effects and an interaction effect.\n",
    "If the interaction effect has a significant p-value, it suggests that the effect of one factor depends on the level of the other factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6844242-96e3-4077-97df-47954c2171f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb073625-f6d5-440c-a773-17dcc5c201de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Sample T-Test:\n",
      "t-statistic: -2.315158728279605\n",
      "p-value: 0.022690065589586535\n",
      "\n",
      "Post-Hoc Tukey HSD Test:\n",
      "   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "=========================================================\n",
      " group1    group2    meandiff p-adj  lower  upper  reject\n",
      "---------------------------------------------------------\n",
      "Control Experimental   5.2768 0.0227 0.7537 9.7998   True\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(123)\n",
    "control_scores = np.random.normal(loc=70, scale=10, size=50)\n",
    "experimental_scores = np.random.normal(loc=75, scale=10, size=50)\n",
    "\n",
    "# Combine data and create group labels\n",
    "data = np.concatenate((control_scores, experimental_scores))\n",
    "groups = np.array(['Control'] * 50 + ['Experimental'] * 50)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print the t-test results\n",
    "print(\"Two-Sample T-Test:\")\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Perform post-hoc Tukey HSD test\n",
    "multi_comp = MultiComparison(data, groups)\n",
    "posthoc_results = multi_comp.tukeyhsd()\n",
    "\n",
    "# Print the post-hoc Tukey HSD test results\n",
    "print(\"\\nPost-Hoc Tukey HSD Test:\")\n",
    "print(posthoc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0360e761-327d-41b9-9c85-25b654994b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77178249-ab2a-4585-8493-561531f4ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Way ANOVA:\n",
      "            df         sum_sq      mean_sq         F    PR(>F)\n",
      "Store      2.0    2359.355556  1179.677778  0.369284  0.692307\n",
      "Residual  87.0  277921.366667  3194.498467       NaN       NaN\n",
      "\n",
      "Post-Hoc Tukey HSD Test:\n",
      " Multiple Comparison of Means - Tukey HSD, FWER=0.05 \n",
      "=====================================================\n",
      "group1 group2 meandiff p-adj   lower    upper  reject\n",
      "-----------------------------------------------------\n",
      "     A      B -11.5333   0.71 -46.3309 23.2643  False\n",
      "     A      C     -1.5 0.9942 -36.2976 33.2976  False\n",
      "     B      C  10.0333 0.7714 -24.7643 44.8309  False\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(123)\n",
    "days = np.arange(1, 31)\n",
    "store_A_sales = np.random.randint(100, 300, size=30)\n",
    "store_B_sales = np.random.randint(150, 350, size=30)\n",
    "store_C_sales = np.random.randint(120, 280, size=30)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Day': np.repeat(days, 3),\n",
    "    'Store': np.tile(['A', 'B', 'C'], 30),\n",
    "    'Sales': np.concatenate([store_A_sales, store_B_sales, store_C_sales])\n",
    "})\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "model = ols('Sales ~ Store', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(\"One-Way ANOVA:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Perform post-hoc Tukey HSD test\n",
    "posthoc = pairwise_tukeyhsd(data['Sales'], data['Store'])\n",
    "\n",
    "# Print the post-hoc Tukey HSD test results\n",
    "print(\"\\nPost-Hoc Tukey HSD Test:\")\n",
    "print(posthoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb801cf-8da6-47f5-9163-a039085d286c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
